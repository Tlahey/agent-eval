# ðŸ§ª AgentEval

**AI coding agent evaluation framework with Vitest-like DX.**

Test, judge, and track AI coding agents â€” locally, sequentially, and model-agnostically.

---

## Features

- **Vitest-like API** â€” `test()` / `expect()` syntax designed for evaluating AI agents
- **Git Isolation** â€” automatic `git reset --hard` between runs for pristine environments
- **LLM-as-a-Judge** â€” structured evaluation via Anthropic, OpenAI, or local Ollama
- **Model Matrix** â€” compare multiple agents/models on the same test suite
- **Data Ledger** â€” JSONL-based historical tracking of all evaluation results
- **CLI** â€” `agenteval run`, `agenteval ledger`, and more

## Why We Built This

### The Paradigm Shift

Testing an AI coding agent is **fundamentally different** from testing a standard JavaScript function.

Traditional software testing evaluates deterministic inputs and outputs in memory. Testing AI coding agents, however, involves long-running tasks, heavy side-effects (mutating real files), and subjective evaluation criteria that require another LLM to act as a judge.

No existing tool was designed for this.

### Why not Vitest or Jest?

While we love the Developer Experience (DX) of Vitest, these frameworks are built for extreme speed and parallel execution. AI agents mutate the actual file system and commit to Git. Running agent tests concurrently in Vitest instantly corrupts the local repository state. Furthermore, agent tasks take minutes to run, conflicting with the millisecond timeouts expected by standard test runners.

### Why not Promptfoo?

[Promptfoo](https://github.com/promptfoo/promptfoo) is an incredible tool for Text-in/Text-out evaluation (like RAGs or Chatbots). However, evaluating code-generating agents requires running CLI commands, capturing Git diffs, and reading compilation logs. Forcing Promptfoo to handle heavy side-effects required fragile workarounds (like complex Bash escaping and JSON parsing hacks). We needed a tool natively designed for file-system operations.

### Why not Langfuse or Cloud LLMOps tools?

[Langfuse](https://langfuse.com/) is perfect for production observability, but it is not a local test runner. Moreover, sending proprietary enterprise code, Next.js build logs, and Git diffs to a third-party cloud service for evaluation raised significant data privacy and security concerns.

### What AgentEval Brings

We built AgentEval to hit the perfect sweet spot:

- **Familiar, Vitest-like syntax** â€” great Developer Experience with `test()` / `expect()` you already know.
- **Strict, sequential execution** â€” automated Git state isolation (`git reset --hard`) between every test. No concurrency corruption.
- **Provider-agnostic architecture** â€” easily switch between local CLIs, OpenAI, or Anthropic for both agents and LLM-as-a-Judge.
- **Local, privacy-first ledger** â€” track historical performance in a JSONL file without sending your source code to the cloud.

---

## Quick Start

### Install

```bash
pnpm add -D agent-eval
```

### Configure

```ts
// agenteval.config.ts
import { defineConfig } from "agent-eval";

export default defineConfig({
  runners: [
    {
      name: "copilot",
      type: "cli",
      command: 'gh copilot suggest "{{prompt}}"',
    },
  ],
  judge: {
    provider: "anthropic",
    model: "claude-sonnet-4-20250514",
  },
});
```

### Write a test

```ts
// tests/banner.eval.ts
import { test, expect } from "agent-eval";

test("Add a Close button to the Banner", async ({ agent, ctx }) => {
  await agent.run("Add a Close button inside the banner component");

  ctx.storeDiff();
  await ctx.runCommand("test", "pnpm test -- Banner");
  await ctx.runCommand("build", "pnpm run build");

  await expect(ctx).toPassJudge({
    criteria: `
      - Uses a proper close button component
      - Has aria-label 'Close'
      - All tests pass
      - Build succeeds
    `,
  });
});
```

### Run

```bash
npx agenteval run
```

---

## Monorepo Structure

```
agent-eval/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ docs/               # VitePress documentation
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ agent-eval/         # agent-eval (core framework)
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ index.ts    # Public API (test, expect, defineConfig)
â”‚           â”œâ”€â”€ core/       # Types, config, context, runner, expect
â”‚           â”œâ”€â”€ git/        # Git isolation (reset, diff)
â”‚           â”œâ”€â”€ judge/      # LLM-as-a-Judge
â”‚           â”œâ”€â”€ ledger/     # JSONL ledger
â”‚           â””â”€â”€ cli/        # CLI binary
â”œâ”€â”€ examples/               # Example config + test files
â”œâ”€â”€ AGENTS.md               # AI agent development guide
â””â”€â”€ PRD.md                  # Product requirements
```

## Development

### Prerequisites

- Node.js â‰¥ 18
- pnpm â‰¥ 10

### Setup

```bash
git clone <repo-url>
cd agent-eval
pnpm install
```

### Commands

| Command | Description |
|---------|-------------|
| `pnpm build` | Build the core package |
| `pnpm test` | Run unit tests (vitest) |
| `pnpm dev` | Start docs dev server |
| `pnpm docs:build` | Build docs for production |
| `pnpm --filter agent-eval typecheck` | Type-check the framework |

### Workflow

1. Make your changes
2. Run `pnpm test` to verify
3. Run `pnpm build` to ensure the build passes
4. Commit when green âœ…

---

## Documentation

Run the docs locally:

```bash
pnpm dev
```

Covers: [Getting Started](apps/docs/guide/getting-started.md) Â· [Configuration](apps/docs/guide/configuration.md) Â· [Writing Tests](apps/docs/guide/writing-tests.md) Â· [Judges](apps/docs/guide/judges.md) Â· [CLI](apps/docs/guide/cli.md) Â· [API Reference](apps/docs/api/)

---

## License

ISC
