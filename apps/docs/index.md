---
layout: home
hero:
  name: AgentEval
  text: AI Agent Evaluation Framework
  tagline: Test, judge, and track AI coding agents with Vitest-like DX
  actions:
    - theme: brand
      text: Get Started
      link: /guide/getting-started
    - theme: alt
      text: API Reference
      link: /api/test
features:
  - title: ğŸ§ª Vitest-like DX
    details: Familiar test() / expect() API designed specifically for evaluating AI agents.
  - title: ğŸ”’ Environment Isolation
    details: Pluggable environment setup (local git reset, Docker containers, or custom) ensures pristine workspaces between tests.
  - title: âš–ï¸ LLM-as-a-Judge
    details: Structured evaluation via Anthropic, OpenAI, Ollama, or any CLI tool as judge.
  - title: ğŸ“Š SQLite Ledger
    details: Local, privacy-first historical tracking with SQL-powered analytics.
  - title: ğŸ“ˆ Visual Dashboard
    details: Built-in React dashboard with score trends, runner comparison, and GitHub-style diff viewer.
  - title: ğŸ—ï¸ SOLID Architecture
    details: Modular, extensible design. Add providers without touching core logic.
---
